<script> MathJax.Hub.Queue(["Typeset",MathJax.Hub]); </script>

# íŒŒì´ì¬ íŒ¨í‚¤ì§€ì™€ ë°ì´í„°

statsmodels íŒ¨í‚¤ì§€ (ìŠ¤íƒ¯ì¸ ëª¨ë¸ì¦ˆ íŒ¨í‚¤ì§€ë¼ê³  ì½ëŠ”ë‹¤.)

scikit-learn íŒ¨í‚¤ì§€ (ì‹¸ì´í‚·-ëŸ° íŒ¨í‚¤ì§€ë¼ê³  ì½ëŠ”ë‹¤.)

### statsmodels íŒ¨í‚¤ì§€

ê²€ì • ë° ì¶”ì • (test and estimation)

íšŒê·€ë¶„ì„ (regression analysis)

ì‹œê³„ì—´ ë¶„ì„ (time-seies analysis)

~~~python
import statsmodels.api as sm
~~~

### scikit-learn íŒ¨í‚¤ì§€

~~~python
import sklearn as sk
~~~

### ê°€ìƒë°ì´í„°

`make_regression()` : íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ê°€ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©

~~~python
X, y = make_regression(n_samples, n_features, bias, noise, random_state)
~~~

~~~python
X, y, w = make_regression(... coef=True)
~~~

`n_samples`: ì •ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 100)

í‘œë³¸ ë°ì´í„°ì˜ ê°¯ìˆ˜ ğ‘N

`n_features` : ì •ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 100)

ë…ë¦½ ë³€ìˆ˜(feature)ì˜ ìˆ˜(ì°¨ì›) ğ‘€M

`bias`: ì‹¤ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 0.0)

y ì ˆí¸

`noise`: ì‹¤ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 0.0)

ì¶œë ¥ ì¦‰, ì¢…ì† ë³€ìˆ˜ì— ë”í•´ì§€ëŠ” ì¡ìŒ ğœ–Ïµì˜ í‘œì¤€í¸ì°¨

`random_state`: ì •ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ None)

ë‚œìˆ˜ ë°œìƒìš© ì‹œë“œê°’

`coef`: ë¶ˆë¦¬ì–¸ (ì˜µì…˜, ë””í´íŠ¸ False)

True ì´ë©´ ì„ í˜• ëª¨í˜•ì˜ ê³„ìˆ˜ë„ ì¶œë ¥

ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤

`X`: [`n_samples`, `n_features`] í˜•ìƒì˜ 2ì°¨ì› ë°°ì—´

ë…ë¦½ ë³€ìˆ˜ì˜ í‘œë³¸ ë°ì´í„° í–‰ë ¬ X

`y`: [`n_samples`] í˜•ìƒì˜ 1ì°¨ì› ë°°ì—´

ì¢…ì† ë³€ìˆ˜ì˜ í‘œë³¸ ë°ì´í„° ë²¡í„° ğ‘¦y

`coef`: [`n_features`] í˜•ìƒì˜ 1ì°¨ì› ë°°ì—´ ë˜ëŠ” [`n_features`, `n_targets`] í˜•ìƒì˜ 2ì°¨ì› ë°°ì—´ (ì˜µì…˜)

ì„ í˜• ëª¨í˜•ì˜ ê³„ìˆ˜ ë²¡í„° ğ‘¤, ì…ë ¥ ì¸ìˆ˜ `coef`ê°€ True ì¸ ê²½ìš°ì—ë§Œ ì¶œë ¥ë¨

#### `make_regression( )` ëª…ë ¹ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ ê³¼ì •ì„ ê±°ì³ ê°€ìƒì˜ ë°ì´í„°ë¥¼ ë§Œë“ ë‹¤.

1. ë…ë¦½ë³€ìˆ˜ ë°ì´í„° í–‰ë ¬ `X`ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§Œë“ ë‹¤.
2. ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ë¥¼ ì—°ê²°í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„° `w`ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§Œë“ ë‹¤.
3. `X`ì™€ `w`ë¥¼ ë‚´ì í•˜ê³  yì ˆí¸ `b` ê°’ì„ ë”í•˜ì—¬ ë…ë¦½ë³€ìˆ˜ì™€ ì™„ì „ì„ í˜•ì¸ ì¢…ì†ë³€ìˆ˜ ë²¡í„° `y_0`ë¥¼ ë§Œë“ ë‹¤.
4. ê¸°ëŒ“ê°’ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ `noise`ì¸ ì •ê·œë¶„í¬ë¥¼ ì´ìš©í•˜ì—¬ ì¡ìŒ `epsilon`ë¥¼ ë§Œë“ ë‹¤.
5. ë…ë¦½ë³€ìˆ˜ì™€ ì™„ì „ì„ í˜•ì¸ ì¢…ì†ë³€ìˆ˜ ë²¡í„° `y_0`ì— ì¡ìŒ `epsilon`ì„ ë”í•´ì„œ ì¢…ì†ë³€ìˆ˜ ë°ì´í„° ğ‘¦ë¥¼ ë§Œë“ ë‹¤.

$$
y = w^Tx + b + \epsilon
$$

#### `make_regression( )` í•¨ìˆ˜ êµ¬í˜„ (coef=True, n_features=1)

~~~python
coef=True, n_features=1
def make_regression(n_sample, bias, noise, random_state=None):
	np.random.seed(random_state)
	n_sample = np.random.rand(n_sample)
	w = np.random.rand()*100
	epsilon = np.random.randn(n_sample)*noise
	y_0 = (n_sample * w) + bias + epsilon
	return n_sample, y_0, w
~~~

`n_features` ì¦‰, ë…ë¦½ ë³€ìˆ˜ê°€ 2ê°œì¸ í‘œë³¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ìŠ¤ìºí„° í”Œë¡¯ì„ ê·¸ë¦¬ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì¢…ì† ë³€ìˆ˜ ê°’ì€ ì ì˜ ëª…ì•”ìœ¼ë¡œ í‘œì‹œí•˜ì˜€ë‹¤.

`make_regression` ëª…ë ¹ì€ ìœ„ì—ì„œ ì„¤ëª…í•œ ì¸ìˆ˜ ì´ì™¸ì—ë„ ë‹¤ìŒê³¼ ê°™ì€ ì¸ìˆ˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

`n_informative`: ì •ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 10)

ë…ë¦½ ë³€ìˆ˜(feature) ì¤‘ ì‹¤ì œë¡œ ì¢…ì† ë³€ìˆ˜ì™€ ìƒê´€ ê´€ê³„ê°€ ìˆëŠ” ë…ë¦½ ë³€ìˆ˜ì˜ ìˆ˜(ì°¨ì›)

`effective_rank`: ì •ìˆ˜ ë˜ëŠ” None (ì˜µì…˜, ë””í´íŠ¸ None)

ë…ë¦½ ë³€ìˆ˜(feature) ì¤‘ ì„œë¡œ ë…ë¦½ì¸ ë…ë¦½ ë³€ìˆ˜ì˜ ìˆ˜. ë§Œì•½ Noneì´ë©´ ëª¨ë‘ ë…ë¦½

`tail_strength`: 0ë¶€í„° 1ì‚¬ì´ì˜ ì‹¤ìˆ˜ (ì˜µì…˜, ë””í´íŠ¸ 0.5)

`effective_rank`ê°€ Noneì´ ì•„ë‹Œ ê²½ìš° ë…ë¦½ ë³€ìˆ˜ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë³€ìˆ˜. 0.5ë©´ ë…ë¦½ ë³€ìˆ˜ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ì—†ë‹¤.



________________________________
###### Reference
ê¹€ë„í˜• ë°•ì‚¬ë‹˜ ê°•ì˜ë¥¼ ìˆ˜ê°•í•˜ë©° ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ìŠ¤ì¿¨(https://datascienceschool.net/) ê°•ì˜ìë£Œë¥¼ í† ëŒ€ë¡œ ê³µë¶€í•˜ë©° ì •ë¦¬í•œ ë‚´ìš©ì„ì„ ë§ì”€ë“œë¦½ë‹ˆë‹¤. 