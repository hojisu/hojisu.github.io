<script> MathJax.Hub.Queue(["Typeset",MathJax.Hub]); </script>

# ì„ í˜• íšŒê·€ë¶„ì„ì˜ ê¸°ì´ˆ

### Summary

- ì„ í˜•íšŒê·€ë¶„ì„ì€ ë…ë¦½ë³€ìˆ˜ xì— ëŒ€ì‘í•˜ëŠ” ì¢…ì†ë³€ìˆ˜ yì™€ ê°€ì¥ ë¹„ìŠ·í•œ ê°’ì„ ì¶œë ¥í•˜ëŠ” ì„ í˜•í•¨ìˆ˜ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤. 
- OLS(Ordinary Least Squares) ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ê²°ì •ë¡ ì  ì„ í˜• íšŒê·€ ë°©ë²•ìœ¼ë¡œ ì”ì°¨ì œê³±í•©(RSS: Residual Sum of Squares)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ í–‰ë ¬ ë¯¸ë¶„ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.

_____________

### íšŒê·€ë¶„ì„ regression analysis

íšŒê·€ë¶„ì„(regression analysis)ì€ Dì°¨ì› ë²¡í„° ë…ë¦½ ë³€ìˆ˜ $$x$$ì™€ ì´ì— ëŒ€ì‘í•˜ëŠ” ìŠ¤ì¹¼ë¼ ì¢…ì†ë³€ìˆ˜ $$y$$ê°„ì˜ ê´€ê³„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì°¾ì•„ë‚´ëŠ” ì‘ì—…ì´ë‹¤.

#### ê²°ì •ë¡ ì  ëª¨í˜•(deterministic Model)

ë…ë¦½ ë³€ìˆ˜ $$x$$ì— ëŒ€í•´ ëŒ€ì‘í•˜ëŠ” ì¢…ì† ë³€ìˆ˜ $$y$$ì™€ ê°€ì¥ ë¹„ìŠ·í•œ ê°’ $$\hat{y}$$ ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ $$f(x)$$ ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤. 

$$
\hat{y} = f \left( x \right) \approx y
$$

ì„ í˜• íšŒê·€ë¶„ì„(linear regression analysis)ì€ ë…ë¦½ ë³€ìˆ˜ $$x$$ì™€ ì´ì— ëŒ€ì‘í•˜ëŠ” ì¢…ì† ë³€ìˆ˜ yê°„ì˜ ê´€ê³„ê°€ ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• í•¨ìˆ˜ $$f(x)$$ ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.  
$$
\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_D x_D = w_0 + w^Tx
$$

$$w_0, \cdots, w_D$$ ë¥¼ í•¨ìˆ˜ $$f(x) $$ ì˜ ê³„ìˆ˜(coefficient) ì´ì ì´ ì„ í˜• íšŒê·€ëª¨í˜•ì˜ ëª¨ìˆ˜(parameter) ë¼ê³  í•œë‹¤.


### ìƒìˆ˜í•­ ê²°í•©

ìƒìˆ˜í•­ ê²°í•©(bias augmentation)ì€ ìƒìˆ˜í•­ì´ 0ì´ ì•„ë‹Œ íšŒê·€ë¶„ì„ëª¨í˜•ì¸ ê²½ìš°ì—ëŠ” ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ìƒìˆ˜í•­ì„ ë…ë¦½ë³€ìˆ˜ì— ì¶”ê°€í•œë‹¤.

$$
x_i =
\begin{bmatrix}
x_{i1} \\ x_{i2} \\ \vdots \\ x_{iD}
\end{bmatrix}
\rightarrow 
x_{i,a} =
\begin{bmatrix}
1 \\ x_{i1} \\ x_{i2} \\ \vdots \\ x_{iD}
\end{bmatrix}
$$

ìƒìˆ˜í•­ ê²°í•©ì„ í•˜ê²Œ ë˜ë©´ ëª¨ë“  ì›ì†Œê°€ 1ì¸ ë²¡í„°ê°€ ì…ë ¥ ë°ì´í„° í–‰ë ¬ì— ì¶”ê°€ëœë‹¤.

$$
X =
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1D} \\
x_{21} & x_{22} & \cdots & x_{2D} \\
\vdots & \vdots & \vdots & \vdots \\
x_{N1} & x_{N2} & \cdots & x_{ND} \\
\end{bmatrix}
\rightarrow 
X_a =
\begin{bmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1D} \\
1 & x_{21} & x_{22} & \cdots & x_{2D} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{N1} & x_{N2} & \cdots & x_{ND} \\
\end{bmatrix}
$$

ì´ë ‡ê²Œ ë˜ë©´ ì „ì²´ ìˆ˜ì‹ì´ ë‹¤ìŒê³¼ ê°™ì´ ìƒìˆ˜í•­ì´ ì¶”ê°€ëœ ê°€ì¤‘ì¹˜ ë²¡í„° ğ‘¤ì™€ ìƒìˆ˜í•­ì´ ì¶”ê°€ëœ ì…ë ¥ ë°ì´í„° ë²¡í„° ğ‘¥ì˜ ë‚´ì ìœ¼ë¡œ ê°„ë‹¨íˆ í‘œì‹œëœë‹¤.

$$
f(x) = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_D x_D
= 
\begin{bmatrix}
1 & x_1 & x_2 & \cdots & x_D
\end{bmatrix}
\begin{bmatrix}
w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_D
\end{bmatrix}
= x_a^T w_a = w_a^T x_a
$$

~~~python
#ìƒìˆ˜í•­ ê²°í•©
X = np.hstack([np.ones((X0.shape[0], 1)), X0])
X[:5]
~~~

~~~python
# statsmodelsì—ëŠ”ìƒìˆ˜í•­ ê²°í•©ì„ ìœ„í•œ `add_constant` í•¨ìˆ˜ê°€ ì œê³µëœë‹¤.
import statsmodels.api as sm

X = sm.add_constant(X0)
X[:5]
~~~

### OLS(Ordinary Least Squares)

OLS(Ordinary Least Squares)ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ê²°ì •ë¡ ì  ì„ í˜• íšŒê·€ ë°©ë²•ìœ¼ë¡œ ì”ì°¨ì œê³±í•©(RSS: Residual Sum of Squares)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ í–‰ë ¬ ë¯¸ë¶„ìœ¼ë¡œ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.

ì˜ˆì¸¡ ëª¨í˜•ì€ ìƒìˆ˜í•­ì´ ê²°í•©ëœ ì„ í˜• ëª¨í˜•ì´ë‹¤.

$$
\hat{y} = Xw
$$

ì”ì°¨ ë²¡í„°(residual vector) $$e$$

$$
e = {y} - \hat{y} = y - Xw
$$

ì”ì°¨ ì œê³±í•©(RSS:residual sum of squares)

$$
\begin{eqnarray}
\text{RSS}
&=&  e^Te \\
&=& (y - Xw)^T(y - Xw) \\
&=& y^Ty - 2y^T X w + w^TX^TXw  
\end{eqnarray}
$$

ì”ì°¨ì˜ í¬ê¸° ì¦‰, ì”ì°¨ ì œê³±í•©ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì”ì°¨ ì œê³±í•©ì˜ ê·¸ë ˆë””ì–¸íŠ¸(gradient) ë²¡í„°ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\dfrac{d \text{RSS}}{d w} = -2 X^T y + 2 X^TX w
$$

ì”ì°¨ê°€ ìµœì†Œê°€ ë˜ëŠ” ìµœì í™” ì¡°ê±´ì€ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ê°€ 0ë²¡í„°ì´ì–´ì•¼ í•˜ë¯€ë¡œ ë‹¤ìŒ ì‹ì´ ì„±ë¦½í•œë‹¤.

$$
\dfrac{d \text{RSS}}{d w}  = 0 \\
X^TX w^{\ast} = X^T y
$$

ë§Œì•½ $$X^TX$$ í–‰ë ¬ì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•œë‹¤ë©´ ë‹¤ìŒì²˜ëŸ¼ ìµœì  ê°€ì¤‘ì¹˜ ë²¡í„° $$w^âˆ—$$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$
w^{\ast} = (X^TX)^{-1} X^T y
$$

$$X^TX$$ í–‰ë ¬ì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ê³  ìœ„ì—ì„œ êµ¬í•œ ê°’ì´ ìµœì €ê°’ì´ ë˜ë ¤ë©´  ì”ì°¨ì œê³±í•©ì˜ í—¤ì‹œì•ˆ í–‰ë ¬ì¸ $$X^TX$$ê°€ ì–‘ì˜ ì •ë¶€í˜¸(positive definite) í–‰ë ¬ì´ì–´ì•¼ í•œë‹¤. 

$$
\frac{d^2 \text{RSS}}{dw^2} = 2X^TX > 0
$$

ë§Œì•½  Xê°€ í’€ë­í¬ê°€ ì•„ë‹ˆë©´ ì¦‰, Xì˜ ê° í–‰ë ¬ì´ ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆë©´ $$X^TX$$ê°€ ì–‘ì˜ ì •ë¶€í˜¸ê°€ ì•„ë‹ˆê³  ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìœ„ì™€ ê°™ì€ í•´ë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤.

### ì§êµ ë°©ì •ì‹

ì§êµ ë°©ì •ì‹(normal equation)ì€ ê·¸ë ˆë””ì–¸íŠ¸ê°€ 0ë²¡í„°ê°€ ë˜ëŠ” ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. 

$$
X^T y - X^TX w = 0
$$

ì§êµ ë°©ì •ì‹ì„ ì¸ìˆ˜ ë¶„í•´í•˜ë©´

$$
X^T (y - X w ) = 0 \\
X^T e = 0
$$

$$c_d$$ê°€ ëª¨ë“  ë°ì´í„°ì˜ $$d$$ë²ˆì§¸ ì°¨ì›ì˜ ì›ì†Œë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ë²¡í„°(íŠ¹ì§• í–‰ë ¬ì˜ ì—´ë²¡í„°)ë¼ê³  í•  ë•Œ ëª¨ë“  ì°¨ì› $$d(d=0,â€¦,D)$$ì— ëŒ€í•´ $$c_d$$ëŠ” ì”ì°¨ ë²¡í„° $$e$$ì™€ ìˆ˜ì§ì„ ì´ë£¬ë‹¤.

$$
c_d^T e = 0 \;\;\; (d=0, \ldots, D) \\
c_d \perp e \;\;\; (d=0, \ldots, D)
$$

ì§êµ ë°©ì •ì‹ìœ¼ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ ì„±ì§ˆì„ ì•Œ ìˆ˜ ìˆë‹¤.

1. ëª¨í˜•ì— ìƒìˆ˜í•­ì´ ìˆëŠ” ê²½ìš°ì— ì”ì°¨ ë²¡í„°ì˜ ì›ì†Œì˜ í•©ì€ 0ì´ë‹¤. ì¦‰, ì”ì°¨ì˜ í‰ê· ì€ 0ì´ë‹¤.

$$
  \sum_{i=0}^N e_i = 0
$$

ìƒìˆ˜í•­ ê²°í•©ì´ ë˜ì–´ ìˆìœ¼ë©´ $$X$$ì˜ ì²«ë²ˆì§¸ ì—´ì´ 1-ë²¡í„°ë¼ëŠ” ê²ƒì„ ì´ìš©í•˜ì—¬ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.

$$
  c_0^T e = \mathbf{1}^T e = \sum_{i=0}^N e_i = 0
$$

2. $$x$$ ë°ì´í„°ì˜ í‰ê· ê°’ $$\bar x$$ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì€ $$y$$ ë°ì´í„°ì˜ í‰ê· ê°’ $$\bar y$$ì´ë‹¤.

$$
\bar{y} = w^T \bar{x}
$$

ì¦ëª…

$$
\begin{eqnarray}
  \bar{y} 
  &=& \dfrac{1}{N}\mathbf{1}^T y \\
  &=& \dfrac{1}{N}\mathbf{1}^T (Xw + e) \\
  &=& \dfrac{1}{N}\mathbf{1}^TXw + \dfrac{1}{N}\mathbf{1}^Te \\
  &=& \dfrac{1}{N}\mathbf{1}^TXw \\
  &=& \dfrac{1}{N}\mathbf{1}^T \begin{bmatrix}c_1 & \cdots & c_M \end{bmatrix} w \\
  &=& \begin{bmatrix}\dfrac{1}{N}\mathbf{1}^Tc_1 & \cdots & \dfrac{1}{N}\mathbf{1}^Tc_D \end{bmatrix} w \\
  &=& \begin{bmatrix}\bar{c}_1 & \cdots & \bar{c}_D \end{bmatrix} w \\
  &=& \bar{x}^T w \\
  \end{eqnarray}
$$

   

### NumPyë¥¼ ì´ìš©í•œ ì„ í˜• íšŒê·€ë¶„ì„

~~~python
from sklearn.datasets import make_regression

bias = 100
X0, y, w = make_regression(
    n_samples=200, n_features=1, bias=bias, noise=10, coef=True, random_state=1
)
X = sm.add_constant(X0)
y = y.reshape(len(y), 1)
w
# array(86.44794301)
~~~

$$
y = 100 + 86.44794301 x + \epsilon
$$

~~~python
# OLS í•´ë¥¼ ì§ì ‘ ì´ìš©í•˜ëŠ” ë°©ë²•
w = np.linalg.inv(X.T @ X) @ X.T @ y
w
# array([[99.79150869],
#       [86.96171201]])
~~~

$$
\hat{y} = 99.79150869 + 86.96171201 x
$$

ì´ ê²°ê³¼ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ ì„ í˜• íšŒê·€ë¥¼ í†µí•´ êµ¬í•œ ê²°ê³¼ëŠ” ì‹¤ì œ(ìì—° ë²•ì¹™)ì™€ **ë¹„ìŠ·í•˜ì§€ë§Œ ì •í™•í•˜ì§€ëŠ” ì•Šë‹¤**ëŠ” ì ì´ë‹¤.