<script> MathJax.Hub.Queue(["Typeset", MathJax.Hub]); </script>

# 모형 결합 Model combining or 앙상블 방법론 Ensemble methods

### Summary

- 앙상블(모형결합)은 복수의 예측 모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다. 
- 단일 모형을 사용할 때 보다 성능 분산이 감소하여 과최적화를 방지한다. 개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상된다. 
- 모형 결합 방법에는 취합방법론과 부스팅 방법론이 있다. 취합방법론은 사용할 모형의 집합이 이미 결정되어 있다. 다수결방법, 배깅, 랜덤포레스트가 있다. 부스팅 방법은 사용할 모형을 점진적으로 늘려간다. 에이다부스트, 그레이던트 부스트가 있다.
- 배깅은 동일한 모형과 모수를 사용하는 대신 트레이닝 데이터를 랜덤하게 선택해서 다수결 모형에 적용한다.
- 랜던포레스트는 의사결정나무를 개별 모형으로 사용하는 모형결합방법이다. 부트스트래핑과 유사하게 다른데이터를 사용해 트레이닝을 시킬 결정나무를 만든다. 좀 더 무작위성을 주기 위해 각 노드마다 랜덤하게 선택된 독립변수들에서 정보획득량을 계산하여 자식노드를 만들도록 한다. 모형들 사이의 상관관계가 줄어 모형의 성능분산을 감소하는 효과가 있다. 포레스트 안에서 각 노드의 독립변수들이 사용되었기에 트리들에서 각각 독립변수들이 얻은 정보획득량을 평균하여 비교하면 독립변수의 중요도를 살펴볼 수 있다. 
- 아이다부스트는 처음에는 약분류기가 random data N개를 트레이닝하고 테스트를 수행한다. 이 때 제대로 분류하지 맛한 데이터들의 가중치가 높아진다. 약분류기들이 한 번에 하나씩 순차적으로 학습시길 때 먼저 학습한 분류기에서 잘못 분류한 것에 대한 정보를 다음 분류기에 학습시 사용하여 이전 분류기의 단점을 보완한다. 즉 이전 분류기가 오분류한 샘플의 가중치를 adative(상호보완적)하게 바꾸어 가며 잘못 분류된 데이터에 더 집중하게 합니다. 이러한 과정을 계속 반복하여 최종적으로 강분류기의 성능이 높아지게 된다. 예측 성능이 조금 낮은 약분류기들을 조합하여 최종적으로 좀 더 좋은 성능을 발휘하는 하나의 강항 강분류기를 만들게 된다. 약분류기들이 adaptive한 방식으로 학습해나가고 이 약분류기들을 조합하여 하나의 강력한 성능을 가진 강분류기를 만들어 내는 것의 특징 때문에 boosting이라는 이름이 붙었다. 
- 그레디언트 부스트는 residual fittin 방식으로 모델을 추가해나가는 방법이다. 기존 예측모형의 잔차들은 기존 모형이 설명하지 못하는 부분이다. 이 부분을 다시 다른 심플한 예측모형을 학습시키고 기존 모형과 결합합니다. 이러한 방법을 계속하면 잔차는 계속해서 줄어들게 되고 트레이닝 셋을 잘 설명하는 예측 모형을 만들 수 있게 됩니다. 이러한 방식은 bias는 상당히 줄일 수 있어서 과최적합이 일어날 수도 있다는 단점이 있습니다. 따라서 sampling, penalizing 등의 정규화 방법을 같이 이용하는 것이 보편적입니다. 
_________

### 취합(aggregation) 방법론 

#### 다수결 방법 Majority Voting

다수결 방법은 가장 단순한 모형 결합 방법으로 전혀 다른 모형도 결합 할 수 있다. 
- Hard voting : 단순 투표, 개별 모형의 결과 기준
- Soft voting : 가중치 투표, 개별 모형의 조건부 확률의 합의 기준

##### 모형 결합을 사용한 성능 향상

독립적인 개별 모형이 정답을 출력할 확률이 $$p$$ 인 경우에 서로 다르고 독립적인 모형을 $$N$$ 개 모아서 다수결 모형을 만들면 정답을 출력할 확률이 다음과 같아진다.

$$
\sum_{k>\frac{N}{2}}^N \binom N k p^k (1-p)^{N-k}
$$

- $$k$$ 는 정답을 맞춘 갯수, N은 전체 데이터 갯수, $$k> \dfrac {N}{2}$$ 은 정답을 맞춘 갯수가 절반 이상
즉, 모형의 수가 많을 수록 성능 향상이 일어날 가능성이 높다.


#### 배깅 Bagging

각각 다른 확률 모형을 사용하기에는 한계가 있으므로 보통은 배깅 방법을 사용하여 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다. 

배깅은 동일한 모형과 모수를 사용하는 대신 트레이닝 데이터를 랜덤하게 선택해서 다수결 모형을 적용한다. 

트레이닝 데이터를 선택하는 방법에 따른 명칭
- 같은 데이터 샘플을 중복사용(replacement)하지 않으면 : Pasting
- 같은 데이터 샘플을 중복사용(replacement)하면 : Bagging
- 데이터가 아니라 다차원 독립 변수 중 일부 차원을 선택하는 경우: Random Subspace
- 데이터 샘플과 독립 변수 차원 모두 일부만 랜덤하게 사용하면 : Random Patches

성능 평가시에는 트레이닝용으로 선택한 데이터가 아닌 다른 데이터를 사용할 수 있다. 이런 데이터를 OOB(Out-Of-Bag) 데이터라고 한다.

#### 랜덤포레스트 Random Forest

랜던포레스트는 의사결정나무(decision tree)를 개별 모형으로 사용하는 모형결합방법이다. 

독립 변수 차원을 랜덤하게 감소시킨 다음 그 중에서 독립 변수를 선택한다. 그러면 개별 모형들 사이의 상관관계가 줄어들기 때문에 모형 성능의 변동이 감소하는 효과가 있다. 이러한 방법을 극단적으로 적용한 것이 Extremely Randomized Trees 모형으로 이 경우에는 각 노드에서 랜덤하게 독립 변수를 선택한다. 

랜덤포레스트 장점은 각 독립 변수의 중요도(feature importance)를 계산할 수 있다. 각 독립 변수들이 얻어낸 information gain의 평균을 비교하여 어떤 독립 변수가 중요한지를 비교한다. 

### 부스팅(Boosting) 방법론

부스팅 방법은 하나의 모형에서 시작하여 모형 집합에 포함할 개별 모형을 하나씩 추가한다. 

모형의 집합은 위원회(commite) $$C$$라고 하고 $$m$$개의 모형을 포함하는 위원회를 $$C_m$$으로 표시한다. 위원회에 들어가는 개별 모형을 약 분류기(weak classifier)라고 하며 $$k$$로 표시한다. 특징은 한번에 하나씩 모형을 추가한다. 

$$m$$ 번째로 위원회에 추가할 개별 모형 $$k_m$$의 선택 기준은 그 전단계의 위원회 $$C_{m-1}$$ 의 성능을 보완하는 것이다. $$C_m$$ 의 최종결정은 다수결 방법을 사용하지 않고 각각의 개별 모형의 출력을 가중치 $$\alpha$$로 가중선형조합한 값을 판별함수로 사용한다. 

부스팅 방법은 이진 분류에서만 사용할 수 있으며 $$y$$ 값은 1 또는 -1의 값을 가진다. 

$$
y = -1 \text{ or } 1 \\
C_{m}(x_i) =  \text{sign} \left( \alpha_1k_1(x_i) + \cdots + \alpha_{m}k_{m}(x_i) \right)
$$

#### 에이다 부스트 adaboost

에이다 부스트는 위원회에 넣을 개별 모형 $$k_m$$을 선별하는 방법으로 학습 데이터 집합의 $$i$$번째 데이터에 가충지 $$w_i$$를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수 $$L$$로 사용한다. 이 손실함수를 최소화하는 모형이 $$k_m$$으로 선택된다.

$$
L_m = \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)
$$

$$I$$ 는 $$k(x_i) \neq y_i$$ 라는 조건이 만족되면 1, 아니면 0을 가지는 지시함수이다. 따라서 틀린 문제에 대한 가중치의 합니다. 

개별 모형 $$k_m$$ 이 선택된 후에는 가중치 $$\alpha_m$$ (모델이 가지는 투표권)를 결정해야 한다. 성적이 좋으면 알파값이 커지고 나쁘면 알파값이 작아진다. 

$$
\epsilon_m = \dfrac{\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)}{\sum_{i=1}^N w_{m,i}} \\
\alpha_m = \frac{1}{2}\log\left( \frac{1 - \epsilon_m}{\epsilon_m}\right)
$$

데이터에 대한 가중치 $$w_{m,i}$$는 최초(m=1)에는 데이터에 대한 같은 값을 가지지만 위원회가 증가하면서 값이 바뀐다. 가중치의 값은 지수함수를 사용하여 위원회 $$C_{m-1}$$이 맞춘 문제는 작게, 틀린 문제는 크게 확대(boosting) 된다. 

$$
w_{m,i} = w_{m-1,i}  \exp (-y_iC_{m-1}) = 
\begin{cases}
w_{m-1,i}e^{-1}  & \text{ if } C_{m-1} = y_i\\
w_{m-1,i}e & \text{ if } C_{m-1} \neq y_i 
\end{cases}
$$

$$m$$ 번째 멤버의 모든 후보에 대해 위 손실함수를 적용하여 가장 값이 작은 후보를 $$m$$ 번째 멤버로 선정한다. 

에이다부스트는 손실함수를 최소화하는 $$C_m$$을 찾아가는 방법이라는 것을 증명할 수 있다.

$$
L_m = \sum_{i=1}^N \exp(−y_i C_m(x_i))
$$

개별 멤버 $$k_m$$ 과 위원회의 관계

$$
C_m(x_i) = \sum_{j=1}^m \alpha_j k_j(x_i) = C_{m-1}(x_i) + \alpha_m k_m(x_i)
$$

이 식을 대입하면 

$$
\begin{eqnarray}
L_m 
&=& \sum_{i=1}^N \exp(−y_i C_m(x_i)) \\
&=& \sum_{i=1}^N \exp\left(−y_iC_{m-1}(x_i) - \alpha_m y_i k_m(x_i) \right) \\
&=& \sum_{i=1}^N \exp(−y_iC_{m-1}(x_i)) \exp\left(-\alpha_m y_i k_m(x_i)\right) \\
&=& \sum_{i=1}^N w_{m,i} \exp\left(-\alpha_m y_i k_m(x_i)\right) \\
\end{eqnarray}
$$

$$y_i$$ 와 $$k_m(x_i)$$ 가 1또는 -1 값만 가질 수 있다는 점을 이용하면, 

$$
\begin{eqnarray}
L_m 
&=& 
e^{-\alpha_m}\sum_{k_m(x_i) = y_i} w_{m,i} + e^{\alpha_m}\sum_{k_m(x_i) \neq y_i} w_{m,i} \\
&=& 
\left(e^{\alpha_m}-e^{-\alpha_m}\right) \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right) + 
e^{-\alpha_m}\sum_{i=1}^N w_{m,i}
\end{eqnarray}
$$

$$L_m$$ 을 최소화하려면 $$\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)$$ 을 최소화하는 $$k_m$$ 함수를 찾은 다음 $$L_m$$ 을 최소화하는 $$\alpha_m$$ 을 찾아야 한다. 

$$
\dfrac{d L_m}{d \alpha_m} = 0
$$

##### 에이다부스트 모형의 정규화

에이다부스트 모형이 과최적화가 되는 경우에는 학습 속도(learning rate) 조정하여 정규화를 할 수 있다. 필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다. 

$$
C_m = C_{m-1} + \mu \alpha_m k_m
$$

#### 그레디언트 부스트

그레디언트 부스트는 변분법(calculus of variations)을 사용한 모형이다. 손실 범함수(loss functional) $$L(y, C_{m-1})$$을 최소화하는 개별 분류함수 $$k_m$$를 찾는다. 이론적으로 가장 최적의 함수는 범함수의 미분이다. 

$$
C_{m} = C_{m-1} - \alpha_m \dfrac{\delta L(y, C_{m-1})}{\delta C_{m-1}} = C_{m-1} + \alpha_m k_m
$$

그레디언트 부스트 모형은 분류/회귀 문제에 상관없이 개별 멤버 모형으로 회귀분석 모형을 사용한다. 가장 많이 사용되는 회귀분석 모형은 의사결정 회귀나무(decision tree regression model) 모형이다. 
- $$-\tfrac{\delta L(y, C_m)}{\delta C_m}$$ 를 목표값으로 개별 멤버 모형 $$k_m$$ 을 찾는다.
- $$\left( y - (C_{m-1} + \alpha_m k_m) \right)^2$$ 를 최소화하는 스텝사이즈 $$\alpha_m$$ 을 찾는다.
- $$C_m = C_{m-1} + \alpha_m k_m$$ 최종 모형을 갱신한다.

만약 손실 범함수가 오차 제곱 형태라면

$$
L(y, C_{m-1}) = \dfrac{1}{2}(y - C_{m-1})^2
$$

범함수의 미분은 실제 목표값 $$y$$ 와 $$C_{m-1}$$ 과의 차이 즉, 잔차(residual)가 된다. 앞에서 놓친 부분은 잔차로 명시한다. 

$$
-\dfrac{dL(y, C_m)}{dC_m} = y - C_{m-1}
$$

Reference
- https://dohk.tistory.com/217
- https://datascienceschool.net/
- https://3months.tistory.com/368