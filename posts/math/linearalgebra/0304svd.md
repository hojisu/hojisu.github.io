<script> MathJax.Hub.Queue(["Typeset",MathJax.Hub]); </script>

# 특잇값 분해 Singular Value Decomposition, SVD

### Summary

- SVD는 𝑁×𝑀크기의 행렬 𝐴를 다음과 같은 3개의 행렬의 곱으로 나타내는 것이다. 특이값, 왼쪽 특이벡터, 오른쪽 특이벡터로 분해 된다. 

- 정방행렬이 아닌 행렬은 고유분해가 불가능하다. 대신 고유분해와 비슷한 특이분해(singular decomposition)를 할 수 있다.

_____________

### 특잇값과 특이벡터

$$N$$x$$M$$크기의 행렬 $$A$$를 다음과 같은 3개의 행렬의 곱으로 나타내는 것을 **특이분해(singular-decomposition)** 또는 **특잇값 분해(singular value decomposition)**라고 한다.

$$
A = U\Sigma V^T
$$

여기에서 U, S, V는 다음 조건을 만족해야 한다.

- $$\Sigma \in \mathbf{R}^{N \times M}$$: 대각성분이 양수인 대각행렬이어야 한다.**큰 수부터 작은 수의 순서로 배열한다.**
- $$U \in \mathbf{R}^{N \times N}$$: 𝑁×𝑁차원 정방행렬. 모든 열벡터가 단위벡터이고 서로 직교해야 한다.(orthonormal).
- $$V \in \mathbf{R}^{M \times M}$$: 𝑀×𝑀차원 정방행렬. 모든 열벡터가 단위벡터이고 서로 직교해야 한다(orthonormal).

위 식을 만족하는 행렬 S의 대각성분들을 **특잇값(singular value)**, 행렬 U의 열벡터들을 **왼쪽 특이벡터(left singular vector)**, 행렬 v의 행벡터들을 **오른쪽 특이벡터(right singular vector)**라고 부른다.

**특잇값의 갯수는 행렬의 열과 행의 갯수 중 작은 값과 같다.** 특이분해된 형태를 구체적으로 쓰면 다음과 같다. 
- 만약 𝑁>𝑀이면 Σ 행렬이 𝑀개의 특잇값(대각성분)을 가지고 다음처럼 아랫 부분이 0행렬이 된다.

$$
A = 
\begin{bmatrix}
\boxed{\,u_1\!\phantom{\dfrac{\raise 5.5em \mathstrut}{\lower 5.5em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_2\!\phantom{\dfrac{\raise 5.5em \mathstrut}{\lower 5.5em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_3\!\phantom{\dfrac{\raise 5.5em \mathstrut}{\lower 5.5em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_4\!\phantom{\dfrac{\raise 5.5em \mathstrut}{\lower 5.5em \mathstrut}}} \!\!\!\!& 
\cdots \!\!\!\!& 
\boxed{u_N\!\phantom{\dfrac{\raise 5.5em \mathstrut}{\lower 5.5em \mathstrut}}} \!\!\!\!\!\!& 
\end{bmatrix}
\begin{bmatrix}
\boxed{\sigma_1 \phantom{\dfrac{}{}} \!\!} & 0 & 0 & \cdots & 0 \\
0 & \boxed{\sigma_2 \phantom{\dfrac{}{}} \!\!} & 0 & \cdots & 0 \\
0 & 0 & \boxed{\sigma_3 \phantom{\dfrac{}{}} \!\!} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots \\
0 & 0 & 0 & \cdots & \boxed{\sigma_M \phantom{\dfrac{}{}} \!\!} \\
0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots &  & \vdots \\
0 & 0 & 0 & \cdots & 0 \\
\end{bmatrix}
\begin{bmatrix}
\boxed{\;\;\;\;\;\;\;\; v_1^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\; v_2^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\vdots \\ 
\boxed{\;\;\;\;\;\;\;\; v_M^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\end{bmatrix}
$$

- 반대로 𝑁<𝑀이면 Σ 행렬이 𝑁개의 특잇값(대각성분)을 가지고 다음처럼 오른쪽 부분이 0행렬이 된다.

$$
A = 
\begin{bmatrix}
\boxed{\,u_1\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_2\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!& 
\cdots \!\!\!\!& 
\boxed{u_N\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!\!\!& 
\end{bmatrix}
\begin{bmatrix}
\boxed{\sigma_1 \phantom{\dfrac{}{}} \!\!} & 0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & \boxed{\sigma_2 \phantom{\dfrac{}{}} \!\!} & 0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & 0 & \boxed{\sigma_3 \phantom{\dfrac{}{}} \!\!} & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & \cdots & \boxed{\sigma_N \phantom{\dfrac{}{}} \!\!} & 0 & \cdots & 0 \\
\end{bmatrix}
\begin{bmatrix}
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; v_1^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; v_2^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; v_3^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; v_4^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\vdots \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; v_M^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\end{bmatrix}
$$

- 행렬의 크기만 표시하면 다음과 같다.
    - N>M 일 경우
    $$
    N\left\{\phantom{\raise 3em \mathstrut\!}\right.
    \overbrace{
    \boxed{
    \raise 3.5em 
    \hspace 2ex A \hspace 2ex
    \lower 3em {}
    }}^{\large M}
    =
    N\left\{\phantom{\raise 3em \mathstrut\!}\right.
    \overbrace{
    \boxed{
    \raise 3.5em 
    \hspace 6.5ex U \hspace 7ex
    \lower 3em {}
    }}^{\large N}
    \overbrace{
    \boxed{
    \raise 3.5em 
    \hspace 2ex \Sigma \hspace 2ex
    \lower 3em {}
    }}^{\large M}
    \overbrace{
    \boxed{
    \raise 1.2em 
    \hspace 1.6ex V \hspace 2ex
    \lower 0.8em {}
    }}^{\large M}
    \!\!\left.\phantom{\raise 0.8em \mathstrut}\right\}M
    $$

    - M>N 일 경우
    $$
    N\left\{\phantom{\raise 0.8em \mathstrut\!}\right.
    \overbrace{
    \boxed{
    \raise 1.2em 
    \hspace 6.5ex A \hspace 7ex
    \lower 0.8em {}
    }}^{\large M}
    =
    N\left\{\phantom{\raise 0.8em \mathstrut\!}\right.
    \overbrace{
    \boxed{
    \raise 1.2em 
    \hspace 1.6ex U \hspace 2ex
    \lower 0.8em {}
    }}^{\large N}
    \overbrace{
    \boxed{
    \raise 1.2em 
    \hspace 8ex \Sigma \hspace 8ex
    \lower 0.8em {}
    }}^{\large M}
    \overbrace{
    \boxed{
    \raise 3.5em 
    \hspace 6.5ex V \hspace 7ex
    \lower 3em {}
    }}^{\large M}
    \!\left.\phantom{\raise 3em \mathstrut}\right\}M
    $$

- 예를 들어 행렬 A는 다음처럼 특이분해 할 수 있다.
  $$
  A = 
  \begin{bmatrix}
  3 & -1 \\
  1 & 3 \\
  1 & 1
  \end{bmatrix}
  $$

  $$
  A = 
  \begin{bmatrix}
   -\frac{1}{\sqrt{6}} &  \frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{6}} \\ 
   -\frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{5}} &  -\frac{2}{\sqrt{30}} \\ 
   -\frac{1}{\sqrt{6}} & 0 &  \frac{5}{\sqrt{30}}
  \end{bmatrix}
  \begin{bmatrix}
  \sqrt{12} & 0        \\
         0  & \sqrt{10} \\
         0 & 0 
  \end{bmatrix}
  \begin{bmatrix}
  -\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
   \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} 
  \end{bmatrix}
  $$


### 특이값 분해의 축소형

**특잇값 대각행렬**에서 **0인 부분은 사실상 아무런 의미가 없기 때문에** **대각행렬의 0 원소부분과 이에 대응하는 왼쪽(혹은 오른쪽) 특이벡터들을 없애**고 다음처럼 **축소된 형태**로 해도 마찬가지로 **원래의 행렬이 나온다.**

- N > M 일 경우, 왼쪽(U) 특이벡터 중에서  $$𝑢_{𝑀+1},⋯,𝑢_𝑁$$을 없앤다.

$$
A = 
\begin{bmatrix}
\boxed{\,u_1\!\phantom{\dfrac{\raise 3em \mathstrut}{\lower 3em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_2\!\phantom{\dfrac{\raise 3em \mathstrut}{\lower 3em \mathstrut}}} \!\!\!\!& 
\cdots \!\!\!\!& 
\boxed{\,u_M\!\phantom{\dfrac{\raise 3em \mathstrut}{\lower 3em \mathstrut}}} \!\!\!\!& 
\end{bmatrix}
\begin{bmatrix}
\boxed{\sigma_1 \phantom{\dfrac{}{}} \!\!} & 0 & 0 & \cdots & 0 \\
0 & \boxed{\sigma_2 \phantom{\dfrac{}{}} \!\!} & 0 & \cdots & 0 \\
0 & 0 & \boxed{\sigma_3 \phantom{\dfrac{}{}} \!\!} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots \\
0 & 0 & 0 & \cdots & \boxed{\sigma_M \phantom{\dfrac{}{}} \!\!} \\
\end{bmatrix}
\begin{bmatrix}
\boxed{\;\;\;\;\;\;\;\; v_1^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\; v_2^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\vdots \\ 
\boxed{\;\;\;\;\;\;\;\; v_M^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;} \\ 
\end{bmatrix}
$$

- M > N 일 경우, 오른쪽(V^T) 특이벡터 중에서 $$𝑣_{𝑁+1},⋯,𝑣_𝑀$$을 없앤다.

$$
A = 
\begin{bmatrix}
\boxed{\,u_1\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!& 
\boxed{\,u_2\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!& 
\cdots \!\!\!\!& 
\boxed{u_N\!\phantom{\dfrac{\raise 2em \mathstrut}{\lower 2em \mathstrut}}} \!\!\!\!\!\!& 
\end{bmatrix}
\begin{bmatrix}
\boxed{\sigma_1 \phantom{\dfrac{}{}} \!\!} & 0 & 0 & \cdots & 0  \\
0 & \boxed{\sigma_2 \phantom{\dfrac{}{}} \!\!} & 0 & \cdots & 0 \\
0 & 0 & \boxed{\sigma_3 \phantom{\dfrac{}{}} \!\!} & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots   \\
0 & 0 & 0 & \cdots & \boxed{\sigma_N \phantom{\dfrac{}{}} \!\!}  \\
\end{bmatrix}
\begin{bmatrix}
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\; v_1^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\; v_2^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\vdots \\ 
\boxed{\;\;\;\;\;\;\;\;\;\;\;\;\; v_N^T \lower 0.3em \mathstrut \;\;\;\;\;\;\;\;\;\;\;\;\;} \\ 
\end{bmatrix}
$$

- 축소형의 경우를 행렬의 크기만 표시하면 다음과 같다.

  - N > M

  $$
  N\left\{\phantom{\raise 3em \mathstrut\!}\right.
  \overbrace{
  \boxed{
  \raise 3.5em 
  \hspace 2ex A \hspace 2ex
  \lower 3em {}
  }}^{\large M}
  =
  N\left\{\phantom{\raise 3em \mathstrut\!}\right.
  \overbrace{
  \boxed{
  \raise 3.5em 
  \hspace 2ex U \hspace 2ex
  \lower 3em {}
  }}^{\large M}
  \overbrace{
  \boxed{
  \raise 1.2em 
  \hspace 1.6ex \Sigma \hspace 2ex
  \lower 0.8em {}
  }}^{\large M}
  \overbrace{
  \boxed{
  \raise 1.2em 
  \hspace 1.6ex V \hspace 2ex
  \lower 0.8em {}
  }}^{\large M}
  \!\!\left.\phantom{\raise 0.8em \mathstrut}\right\}M
  $$

  

  - M > N

$$
N\left\{\phantom{\raise 0.8em \mathstrut\!}\right.
\overbrace{
\boxed{
\raise 1.2em 
\hspace 6.5ex A \hspace 7ex
\lower 0.8em {}
}}^{\large M}
=
N\left\{\phantom{\raise 0.8em \mathstrut\!}\right.
\overbrace{
\boxed{
\raise 1.2em 
\hspace 1.6ex U \hspace 2ex
\lower 0.8em {}
}}^{\large N}
\overbrace{
\boxed{
\raise 1.2em 
\hspace 1.6ex \Sigma \hspace 2ex
\lower 0.8em {}
}}^{\large N}
\overbrace{
\boxed{
\raise 1.2em 
\hspace 6.5ex V \hspace 7ex
\lower 0.8em {}
}}^{\large M}
\!\left.\phantom{\raise 0.8em \mathstrut}\right\}N
$$

- 예를 들어 행렬 A의 특이분해 축소형은 다음과 같다.
  $$
  A = 
  \begin{bmatrix}
   -\frac{1}{\sqrt{6}} &  \frac{2}{\sqrt{5}} \\ 
   -\frac{2}{\sqrt{6}} & -\frac{1}{\sqrt{5}} \\ 
   -\frac{1}{\sqrt{6}} & 0 
  \end{bmatrix}
  \begin{bmatrix}
  \sqrt{12} & 0        \\
         0  & \sqrt{10} \\
  \end{bmatrix}
  \begin{bmatrix}
  -\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\
   \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} 
  \end{bmatrix}
  $$


### 특이분해의 존재

이러한 특이분해는 모든 행렬에 대해 가능하다. 즉 **어떤 행렬이 주어지더라도 위와 같이 특이분해할 수 있다.** 


### 파이썬을 사용한 특이분해

numpy.linalg 서브패키지와 scipy.linalg 서브패키지에서는 특이분해를 할 수 있는 `svd`명령을 제공한다. **오른쪽 특이행렬은 전치행렬로 출력**된다는 점에 주의하라.

~~~python
from numpy.linalg import svd

A = np.array([[3, -1], [1, 3], [1, 1]])
U, S, VT = svd(A)
~~~

~~~python
U
~~~

~~~
# U의 (왼쪽 특이벡터)
array([[-4.08248290e-01,  8.94427191e-01, -1.82574186e-01],
       [-8.16496581e-01, -4.47213595e-01, -3.65148372e-01],
       [-4.08248290e-01, -2.06937879e-16,  9.12870929e-01]])
~~~

~~~python
S
~~~

~~~~
# S의 (특잇값)
array([3.46410162, 3.16227766])
~~~~

~~~python
np.diag(S, 1)[:, 1:]
~~~

~~~
# np.diag(S, 1)[:, 1:] (행, 렬 수를 맞춰주기 위해서)
array([[3.46410162, 0.        ],
       [0.        , 3.16227766],
       [0.        , 0.        ]])
~~~

~~~python
VT
~~~

~~~
# VT  (오른쪽 특이벡터, 결과는 전치행렬로 출력된다.)
array([[-0.70710678, -0.70710678],
       [ 0.70710678, -0.70710678]])
~~~

~~~python
U @ np.diag(S, 1)[:, 1:] @ VT
~~~

~~~
# U @ np.diag(S, 1)[:, 1:] @ VT 의 결과 (특이분해 결과, 원래의 행렬이 출력된다.)
array([[ 3., -1.],
       [ 1.,  3.],
       [ 1.,  1.]])
~~~

- 축소형을 구하려면 인수 `full_matrices=False`로 지정한다.

~~~python
U2, S2, VT2 = svd(A, full_matrices=False)
~~~

~~~python
U2
~~~

~~~
# U2 (왼쪽특이벡터)
array([[-4.08248290e-01,  8.94427191e-01],
       [-8.16496581e-01, -4.47213595e-01],
       [-4.08248290e-01, -2.06937879e-16]])
~~~

~~~python
S2
~~~

~~~
#S2 (특잇값)
array([3.46410162, 3.16227766])
~~~

~~~python
VT2
~~~

~~~
#VT2 (오른쪽 특이벡터)
array([[-0.70710678, -0.70710678],
       [ 0.70710678, -0.70710678]])
~~~

~~~~python
U2 @ np.diag(S2) @ VT2
~~~~

~~~
array([[ 3., -1.],
       [ 1.,  3.],
       [ 1.,  1.]])
~~~



### 특잇값과 특이벡터의 관계

**행렬 𝑉는 정규직교(orthonormal)행렬**이므로 **전치행렬이 역행렬**이다.
$$
V^T = V^{-1}
$$

**정방 행렬 A에 대한 역행렬은 원래의 행렬 A와 다음 관계를 만족하는 정방 행렬을 말한다. I는 항등 행렬(identity matrix) 이다.**
$$
A^{-1} A = A A^{-1} = I
$$

$$
V^T V = V V^T = I
$$

$$
V^{-1} = V^T
$$

특이분해된 등식의 양변에 𝑉를 곱하면,

$$
AV = U\Sigma V^TV = U\Sigma
$$

$$
A 
\begin{bmatrix}
v_1 & v_2 & \cdots & v_M
\end{bmatrix}
= 
\begin{bmatrix}
u_1 & u_2 & \cdots & u_N
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & 0 & \cdots \\
0 & \sigma_2 & \cdots \\
\vdots & \vdots & \ddots \\
\end{bmatrix}
$$

행렬 𝐴를 곱하여 정리하면 𝑀 > N 때는

$$
\begin{bmatrix}
Av_1 & Av_2 & \cdots & Av_N
\end{bmatrix}
= 
\begin{bmatrix}
\sigma_1u_1 & \sigma_2u_2 & \cdots & \sigma_Nu_N
\end{bmatrix}
$$

N > M 일 때는

$$
\begin{bmatrix}
Av_1 & Av_2 & \cdots & Av_M
\end{bmatrix}
= 
\begin{bmatrix}
\sigma_1u_1 & \sigma_2u_2 & \cdots & \sigma_Mu_M
\end{bmatrix}
$$

즉, **𝑖번째 특잇값 𝜎𝑖와 특이벡터 𝑢𝑖, 𝑣𝑖는 다음과 같은 관계가 있다.**

$$
Av_i = \sigma_i u_i \;\; (i=1, \ldots, \text{min}(M,N))
$$

​이 관계는 고유분해와 비슷하지만 고유분해와는 달리 **좌변과 우변의 벡터가 다르다**.

위에서 예로 들었던 행렬의 경우 아래와 같이 성립한다. (좌변(v)과 우변(u)의 벡터가 다르다.)
  $$
  Av_1 = \sigma_1u_1
  $$

  $$
  \begin{bmatrix}
  3 & -1 \\
  1 & 3 \\
  1 & 1
  \end{bmatrix}
  \begin{bmatrix}
  -\frac{1}{\sqrt{2}} \\
  -\frac{1}{\sqrt{2}} 
  \end{bmatrix}
  =
  \sqrt{12}
  \begin{bmatrix}
  -\frac{1}{\sqrt{6}} \\ -\frac{2}{\sqrt{6}} \\ -\frac{1}{\sqrt{6}}
  \end{bmatrix}
  $$

  $$
  Av_2 = \sigma_2u_2
  $$

$$
\begin{bmatrix}
3 & -1 \\
1 & 3 \\
1 & 1
\end{bmatrix}
\begin{bmatrix}
 \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} 
\end{bmatrix}
=
\sqrt{10}
\begin{bmatrix}
 \frac{2}{\sqrt{5}} \\  
-\frac{1}{\sqrt{5}} \\ 
 0 \\ 
\end{bmatrix}
$$


### 특이분해와 고유분해의 관계

행렬 𝐴의 공분산행렬 $$𝐴^𝑇𝐴$$는

$$
\begin{eqnarray}
A^TA^{} 
&=& (V^{} \Sigma^T U^T)( U^{}\Sigma^{} V^T) \\
&=& V^{} \Lambda^{} V^T \\
\end{eqnarray}
$$

​가 되어 행렬 𝐴의 특잇값의 제곱(과 0)이 공분산행렬 $$𝐴^𝑇𝐴$$의 고유값, **행렬 𝐴의 오른쪽 특이벡터가 공분산행렬 $$𝐴^𝑇𝐴$$의 고유벡터**가 된다.

위 식에서 Λ은 𝑁이 𝑀보다 크면 아래와 같다.

$$
\Lambda
= 
\begin{bmatrix}
\sigma_1^2 & 0 & \cdots & 0 \\
0 & \sigma_2^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_M^2  \\
\end{bmatrix}
$$

위 식에서 Λ은 𝑁이 𝑀보다 작으면 아래와 같다.

$$
\Lambda
= 
\begin{bmatrix}
\sigma_1^2 & 0 & \cdots & 0 & \cdots & 0 \\
0 & \sigma_2^2 & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sigma_N^2 & \cdots & 0 \\
\vdots & \vdots & \cdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & \cdots & 0 \\
\end{bmatrix}
= \text{diag}(\sigma_1^2, \sigma_2^2, \cdots, \sigma_N^2, 0, \cdots, 0)
$$

마찬가지 방법으로 **행렬 𝐴의 왼쪽 특이벡터가 공분산행렬 $$𝐴𝐴^𝑇$$의 고유벡터**가 된다는 것을 증명할 수 있다.

Numpy 로 증명

  ~~~python
  w, V = np.linalg.eig(A.T @ A)
  ~~~

  ~~~python
  w  # A.T A의 고윳값
  # 결과 array([12., 10.])
  ~~~

  ~~~python
  S ** 2  # A의 특잇값의 제곱
  # 결과 array([12., 10.])
  ~~~

  ~~~python
  V  # A.T A의 고유벡터
  # 결과 
  # array([[ 0.70710678, -0.70710678],
  #        [ 0.70710678,  0.70710678]])
  ~~~

  ~~~python
  VT.T  # A의 오른쪽 특이벡터
  # 결과
  # array([[-0.70710678,  0.70710678],
  #        [-0.70710678, -0.70710678]])
  ~~~


### 1차원 근사

2차원 평면위에 3개의 2차원 벡터 $$𝑎_1,𝑎_2,𝑎_3$$가 있다. 원점을 지나면서 모든 점들과 가능한한 가까이 있는 직선을 만들고 싶다면 직선의 방향을 어떻게 해야 할까? 직선의 방향을 나타내는 단위 벡터를 𝑤라고 하자.

![image-20190417204135682](../../../resource/img/image-20190417204135682.png)

벡터 𝑤와 점 𝑎𝑖의 거리의 제곱은 다음처럼 계산할 수 있다.

$$
\Vert a_i^{\perp w}\Vert^2 = \Vert a_i\Vert^2 - \Vert a_i^{\Vert w}\Vert^2 = \Vert a_i\Vert^2 - (a_i^Tw)^2
$$

벡터 $$a_1, a_2, a_3$$를 행벡터로 가지는 행렬 A를 가정하면 

$$
\begin{align}
A = \begin{bmatrix} a_1^T \\ a_2^T \\ a_3^T \end{bmatrix}
\end{align}
$$

행벡터의 놈의 제곱의 합은 행렬의 놈이므로 모든 점들과의 거리의 제곱의 합은 행렬의 놈으로 계산된다.

$$
\begin{align}
\begin{aligned}
\sum_{i=1}^3 \Vert a_i^{\perp w}\Vert^2 
&= \sum_{i=1}^3 \Vert a_i\Vert^2 - \sum_{i=1}^3  (a_i^Tw)^2 \\
&= \Vert A \Vert^2 - \Vert Aw\Vert^2 \\
\end{aligned}
\end{align}
$$

점 $$a_i$$의 위치가 고정되어 있으므로 행렬 A의 놈 값은 고정되어 있다. 그래서 이 값이 가장 작아지려면 $$\Vert A \Vert$$값이 가장 크게 만드는 w를 찾으면 된다. 수식으로 표현하자면 $$\arg\max_w \Vert Aw \Vert^2$$ 이다.
_______________

(참고 내용1)
만약 v가 원점을 지나는 직선의 방향을 나타내는 단위 벡터라고 하자. 이 때 그 직선위에 있지 않는 어떤 점 x와 그 직선과의 거리의 제곱이 다음과 같다.

직선위에 있지 않는 어떤 점 x와 그 직선과의 거리는 피타고라스의 정의에 의해 다음과 같다.

$$
\Vert x^{\perp v}\Vert^2 = \Vert x\Vert^2 - \Vert x^{\Vert v}\Vert^2
$$

이 때 직교성분의 길이를 구하면 v가 단위벡터라는 점을 이용하면 다음과 같다.

$$
\| x^{\Vert v} \| 
= \dfrac{x^Tv}{\|v\|} 
= x^Tv
$$

따라서 직선위에 있지 않는 어떤 점 x와 그 직선과의 거리의 제곱이 다음과 같다. 

$$
\Vert x^{\perp v}\Vert^2 = \| x \|^2 - (x^Tv)^2
$$

(참고내용2)
행렬 𝐴($$ A\in \mathbf{R}^{N \times M}$$)의 놈의 제곱$$ ‖𝐴‖^2$$이 그 행렬을 이루는 행 벡터 $$𝑟_𝑖$$의 놈의 제곱의 합 또는 열 벡터 $$𝑐_𝑖$$의 놈의 제곱의 합과 같다.

풀이;
$$
\Vert A \Vert^2 = \sum_{i=1}^N \Vert r_i \Vert^2  = \sum_{j=1}^M \Vert c_j \Vert^2
$$

$$
\Vert A \Vert^2 ={\sum_{i=1}^N \sum_{j=1}^M a_{ij}^2}={\sum_{i=1}^M \sum_{j=1}^N a_{ij}^2}
$$

$$
\Vert r_i \Vert^2 = \sum_{j=1}^M \Vert a_{i,j}\Vert^2
$$

$$
\Vert A \Vert^2 =\sum_{i=1}^N \left( \sum_{j=1}^M a_{ij}^2 \right) = \sum_{i=1}^N \Vert r_i \Vert^2
$$

$$
\Vert c_j \Vert^2 = \sum_{i=1}^N \Vert a_{i,j}\Vert^2
$$

$$
\Vert A \Vert^2 =\sum_{j=1}^M \left( \sum_{i=1}^N a_{ij}^2 \right) = \sum_{j=1}^M \Vert c_i \Vert^2
$$


________________________

### 1차원 근사의 풀이

위에서 예로 든 행렬 $$A \in \mathbf{R}^{3 \times 2}$$를 특이분해하면 2개의 특잇값, 왼쪽/오른쪽 특이벡터를 가진다. 이를 각각 다음처럼 이름붙인다.

첫번째 특잇값: $$\sigma_1$$, 첫번째 왼쪽 특이벡터 $$u_1 \in \mathbf{R}^{3}$$, 첫번째 오른쪽 특이벡터 $$v_1 \in \mathbf{R}^{2}$$
두번째 특잇값: $$\sigma_2$$, 두번째 왼쪽 특이벡터 $$u_2 \in \mathbf{R}^{3}$$, 두번째 오른쪽 특이벡터 $$v_2 \in \mathbf{R}^{2}$$

1) 첫번째 특잇값 $$\sigma_1$$은 두번째 특잇값 $$\sigma_2$$보다 같거나 크다.
$$
\sigma_1 \geq \sigma_2
$$
2) A에 오른쪽 특이벡터를 곱하면 왼쪽 특이벡터 방향이 된다.
$$
A v_1 = \sigma_1 u_1
$$

$$
A v_2 = \sigma_2 u_2
$$

3) 오른쪽 특이벡터 $$𝑣_1,𝑣_2$$는 서로 직교하므로 (같은 방향이 아니라서) 선형독립이고 2차원 평면공간의 기저벡터가 될 수 있다.

4) 우리는 ‖𝐴𝑤‖의 값이 가장 크게 만드는 𝑤를 찾아야 하는데 𝑤는 2차원 벡터이므로 2차원 평면공간의 기저벡터인 $$v_1, v_2$$의 선형조합으로 표현할 수 있다.
$$
w = w_{1} v_1 + w_{2} v_2
$$
  단, 𝑤도 단위벡터이므로 $$w_1,w_2$$는 다음 조건을 만족해야 한다.
$$
w_{1}^2 + w_{2}^2 = 1
$$
5) 이 때 ‖𝐴𝑤‖의 값은
$$
\begin{eqnarray}
\Vert Aw\Vert^2 
&=& \Vert A(w_{1} v_1 + w_{2} v_2)\Vert^2 \\
&=& \Vert w_{1}Av_1 + w_{2}Av_2 \Vert^2 \\
&=& \Vert w_{1} \sigma_1 u_1 + w_{2} \sigma_2 u_2 \Vert^2 \\
&=& \Vert w_{1} \sigma_1 u_1 \Vert^2 + \Vert w_{2} \sigma_2 u_2 \Vert^2 \;\; \text{(벡터의 직교)} \\
&=& w_{1}^2 \sigma_1^2 \Vert  u_1 \Vert^2 + w_{2}^2 \sigma_2^2 \Vert  u_2 \Vert^2 \\
&=& w_{1}^2 \sigma_1^2 + w_{2}^2 \sigma_2^2  \;\; \text{(단위벡터)}\\
\end{eqnarray}
$$
6) 𝜎1>𝜎2>0이므로 $$w_{1}^2 + w_{2}^2 = 1$$라는 조건을 만족하면서 위 값을 가장 크게하는 $$w_1, w_2$$값은
$$
w_{1} = 1, w_{2} = 0
$$
7) 즉, 첫번째 **오른쪽 특이벡터 방향**으로 하는 것이다.
$$
w = v_1
$$
8) 이 때 ‖𝐴𝑤‖는 첫번째 특잇값이 된다.
$$
\Vert Aw\Vert = \Vert Av_1\Vert = \Vert \sigma_1 u_1\Vert = \sigma_1 \Vert u_1\Vert = \sigma_1
$$

위에서 예로 들었던 행렬 A의 경우에는
  $$
  A = 
  \begin{bmatrix}
  3 & -1 \\
  1 & 3 \\
  1 & 1
  \end{bmatrix}
  $$

첫번째 오른쪽 특이벡터가 가장 거리의 합이 작은 방향이 된다. 
  $$
  v_1 = 
  \begin{bmatrix}
  \frac{\sqrt{2}}{2} \\
  \frac{\sqrt{2}}{2} \\
  \end{bmatrix}
  $$

이 때의 거리의 제곱의 합은 다음과 같다.
  $$
  \Vert A \Vert^2 - \Vert Aw\Vert^2
  =\Vert A \Vert^2 - \sigma_1^2
  $$


### 일반적인 풀이

$$
\begin{eqnarray}
\Vert Aw \Vert^2 
&=& \sum_{i=1}^{N}  (a_i^Tw)^2 \\
&=& \sum_{i=1}^{N}  (a_i^Tw)^T(a_i^Tw) \\
&=& \sum_{i=1}^{N}  w^Ta_ia_i^Tw \\
&=& w^T \left( \sum_{i=1}^{N}  a_ia_i^T \right) w \\
&=& w^T A^TA w \\
\end{eqnarray}
$$

공분산행렬의 고유분해 공식을 이용하면 아래와 같다.
  $$
  \begin{eqnarray}
  w^T A^TA w 
  &=& w^T V \Lambda V^T w \\
  &=& w^T \left( \sum_{i=1}^{M}  \sigma^2_iv_iv_i^T \right) w \\
  &=& \sum_{i=1}^{M}\sigma^2_i(w^Tv_i)(v_i^Tw) \\
  &=& \sum_{i=1}^{M}\sigma^2_i\Vert v_i^Tw\Vert^2 \\
  \end{eqnarray}
  $$

이 식에서 M은 0이 아닌 특잇값의 갯수이다. 즉 우리가 풀어야 할 문제는 다음과 같다.

  $$
  \arg\max_w \Vert Aw \Vert^2 = \arg\max_w \sum_{i=1}^{M}\sigma^2_i\Vert v_i^Tw\Vert^2
  $$
  
  위 값을 가장 크게 하려면 w를 가장 큰 특잇값에 대응하는 오른쪽 고유벡터 $$v_1$$으로 해야한다.


### 랭크-1 근사문제

$$𝑎_𝑖$$를 $$w$$에 투영한 벡터는 아래와 같다.

$$
a^{\Vert w}_i = (a_i^Tw)w
$$

$$w$$ 벡터를 이용하면 $$N$$개의 $$M$$차원 벡터 $$a_1, a_2, \cdots, a_N\;(a_i \in \mathbf{R}^M)$$를 1차원으로 투영(projection)하여 가장 비슷한 𝑁개의 1차원 벡터 를$$a^{\Vert w}_1, a^{\Vert w}_2, \cdots, a^{\Vert w}_N\;(a^{\Vert w}_i \in \mathbf{R}^1)$$ 만들 수 있다

$$
A'=
\begin{bmatrix}
\left(a^{\Vert w}_1\right)^T \\
\left(a^{\Vert w}_2\right)^T \\
\vdots \\
\left(a^{\Vert w}_N\right)^T
\end{bmatrix}
=
\begin{bmatrix}
a_1^Tw^{}w^T \\
a_2^Tw^{}w^T \\
\vdots \\
a_N^Tw^{}w^T
\end{bmatrix}
=
\begin{bmatrix}
a_1^T \\
a_2^T \\
\vdots \\
a_N^T
\end{bmatrix}
w^{}w^T
=
Aw^{}w^T
$$

이 답은 원래 행렬 𝐴에 랭크-1 행렬 $$ww^T$$를 곱해서 원래의 행렬 𝐴와 가장 비슷한 행렬 𝐴′을 만드는 문제와 같다.

$$
\arg\min_w \Vert A - A' \Vert = \arg\min_w \Vert A^{} - A^{}w^{}w^T \Vert
$$

위의 문제를 **랭크-1 근사문제(rank-1 approximation problem)**라고도 한다.



### K 차원 근사

이번에는 𝑁개의 𝑀차원 벡터 $$a_1, a_2, \cdots, a_N\;(a_i \in \mathbf{R}^M)$$를 1차원이 아니라 정규직교인 기저벡터 $$w_1, w_2, \cdots, w_K$$로 이루어진 𝐾차원 벡터공간으로 투영하여 가장 비슷한 𝑁개의 𝐾차원 벡터 $$a^{\Vert w}_1, a^{\Vert w}_2, \cdots, a^{\Vert w}_N\;$$를 만들기 위한 정규직교 기저벡터 $$w_1, w_2, \cdots, w_K$$를 찾는 문제를 생각하자. 이 문제는 **랭크-𝐾근사문제**라고 한다.

기저벡터행렬을 𝑊라고 하자.
  $$
  W = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix}
  $$

정규직교 기저벡터에 대한 벡터 $$𝑎_𝑖$$의 투영 $$a^{\Vert w}_i$$는 각 기저벡터에 대한 내적으로 만들 수 있다.

$$
\begin{eqnarray}
a^{\Vert w}_i 
&=& 
(a_i^Tw_1)w_1 +
(a_i^Tw_2)w_2 +
\cdots
+ (a_i^Tw_K)w_K \\
\end{eqnarray}
=
\sum_{k=1}^K (a_i^Tw_k)w_k
$$

벡터 𝑎1,𝑎2,⋯,𝑎𝑁를 행벡터로 가지는 행렬 𝐴를 가정하면
  $$
  A = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_N^T \end{bmatrix}
  $$

모든 점들과의 거리의 제곱의 합은 다음처럼 행렬의 놈으로 계산할 수 있다.

$$
\begin{eqnarray}
\sum_{i=1}^N \Vert a_i^{\perp w}\Vert^2 
&=& \sum_{i=1}^N \Vert a_i\Vert^2 - \sum_{i=1}^N \Vert a^{\Vert w}_i\Vert^2 \\
&=& \Vert A \Vert^2 - \sum_{i=1}^N \Vert a^{\Vert w}_i\Vert^2 \\
\end{eqnarray}
$$

행렬 𝐴는 이미 주어져있으므로 이 값을 가장 작게 하려면 두번째 항의 값을 가장 크게 하면 된다. 두번째 항은 K=1일 때와 같은 방법으로 공분산행렬 형태로 바꿀 수 있다.

$$
\begin{eqnarray}
\sum_{i=1}^N \Vert a^{\Vert w}_i\Vert^2 
&=& \sum_{i=1}^N \sum_{k=1}^K \Vert (a_i^Tw_k)w_k \Vert^2 \\
&=& \sum_{i=1}^N \sum_{k=1}^K \Vert a_i^Tw_k \Vert^2 \\
&=& \sum_{k=1}^K w_k^T A^TA w_k \\
\end{eqnarray}
$$

공분산행렬의 고유분해를 사용하면

$$
\begin{eqnarray}
\sum_{k=1}^K w_k^T A^TA w_k 
&=& \sum_{k=1}^K w_k^T V \Lambda V^T w_k \\
&=& \sum_{k=1}^K w_k^T \left( \sum_{i=1}^{M}  \sigma^2_iv_iv_i^T \right) w_k \\
&=& \sum_{k=1}^K \sum_{i=1}^{M}\sigma^2_i\Vert v_i^Tw_k\Vert^2 \\
\end{eqnarray}
$$

**가장 큰 𝐾개의 특잇값에 대응하는 오른쪽 특이벡터가 기저벡터일 때 가장 값이 커진다.**



### 랭크-K 근사문제

우리가 찾아야 하는 것은 이 값을 가장 크게 하는 𝐾개의 영벡터가 아닌 직교하는 단위벡터 𝑤𝑘이다. 고유분해의 성질로부터 **오른쪽 기저벡터 중 가장 큰 𝐾개의 특잇값에 대응하는 오른쪽 특이벡터가 우리가 찾는 기저벡터가 된다.**

랭크-𝐾 근사문제의 형태로 만들 수도 있다.
  $$
  \begin{eqnarray}
  a^{\Vert w}_i 
  &=& 
  (a_i^Tw_1)w_1 +
  (a_i^Tw_2)w_2 +
  \cdots
  + (a_i^Tw_K)w_K \\
  &=&
  \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix}
  \begin{bmatrix} a_i^Tw_1 \\ a_i^Tw_2 \\ \vdots \\ a_i^Tw_K \end{bmatrix} \\
  &=&
  \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix}
  \begin{bmatrix} w_1^T \\ w_2^T \\ \vdots \\ w_K^T \end{bmatrix} a_i \\
  &=&
  WW^Ta_i
  \end{eqnarray}
  $$

이러한 투영벡터를 모아놓은 행렬 𝐴′는

$$
A'=
\begin{bmatrix}
\left(a^{\Vert w}_1\right)^T \\
\left(a^{\Vert w}_2\right)^T \\
\vdots \\
\left(a^{\Vert w}_N\right)^T
\end{bmatrix}
=
\begin{bmatrix}
a_1^TW^{}W^T \\
a_2^TW^{}W^T\\
\vdots \\
a_N^TW^{}W^T
\end{bmatrix}
=
\begin{bmatrix}
a_1^T \\
a_2^T \\
\vdots \\
a_N^T
\end{bmatrix}
W^{}W^T
=
AW^{}W^T
$$

이 문제는 원래 행렬 𝐴에 랭크-K 행렬 $$WW^T$$를 곱해서 원래의 행렬 𝐴와 가장 비슷한 행렬 𝐴′을 만드는 문제와 같다

$$
\arg\min_{w_1,\cdots,w_K} \Vert A - AW^{}W^T \Vert
$$

