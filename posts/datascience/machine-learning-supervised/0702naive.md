<script> MathJax.Hub.Queue(["Typeset",MathJax.Hub]); </script>

## 나이브베이즈 분류모형

### Summary

- 나이브 베이즈 분류모형은 text 데이터 분석에서 많이 사용된다.
- 나이브 가정은 모든 차원의 개별 독립변수가 서로 조건부 독립이라고 가정한다. 
- 장점은 단순하고 빠르며 매우 효과적이다. 노이즈와 결측 데이터가 있어도 잘 수행한다. 훈련에 대한 상대적으로 적은 예제가 필요하지만 매우 많은 예제도 잘 수행한다. 예측에 대한 추정된 확률을 얻기 쉽다. 메모리 용량을 크게 차지 하지 않는다. 그럼에도 상당히 좋은 성능을 낸다.(SVM과도 견줄만큼 우수한 성능)
_____________

### 조건부 독립(conditional independence)

조건부 독립은 일반적인 독립과 달리 조건이 되는 별개의 확률번수 C가 존재해야 한다. 조건이 되는 확률변수 C에 대한 A, B의 결합조건부확률이 C에 대한 A,B의 조건부확률의 곱과 같으면 A와 B가 C에 대한 조건부독립이라고 한다. 

$$
P(A,B|C) = P(A|C)P(B|C)
$$

$$
A \text{⫫} B \;\vert\; C
$$

A, B가 C에 대해 조건부독립이면 다음도 만족한다.

$$
P(A|B,C) = P(A|C) \\
P(B|A,C) = P(B|C)
$$

### 나이브 가정(naive assumption)

나이브 가정은 모든 차원의 개별 독립변수가 서로 조건부독립이라고 가정한다. 벡터 $$x의$$ 결합확률분포함수는 개별 스칼라 원소 $$x_d$$의 확률분포함수의 곱이 된다. 스칼라 원소 $$x_d$$ 의 확률분포함수는 결합확률분포함수 보다 추정하기 훨씬 쉬워진다. 

$$
P(x_1, \ldots, x_D \mid y = k) = \prod_{d=1}^D P(x_d \mid y = k)
$$

나이브 베이즈 모형의 수식

$$
\begin{align}
P(y = k \mid x) 
&= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\
&= \dfrac{ \left( \prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)}
\end{align}
$$

### 정규분포 가능도 모형

$$x$$ 벡터의 원소가 모두 실수이고 클래스마다 특정한 값 주변에서 발생한다고 하면 가능도 분포로 정규분포를 사용한다. 각 독립변수 $$x$$ 별, 클래스 $$k$$ 마다 정규 분포의 기댓값 $$\mu_{d,k}$$, 표준편차 $$\sigma^2_{d,k}$$ 가 달라진다. 

QDA와 달리 조건부 독립이기 때문에 $$\Sigma$$ (공분산행렬)을 구할 필요가 없다. 그 이유는 비대각 성분이 모두 0이기 때문에 분산만 구하면 된다. 그래서 다변수 정규분포가 아닌 단변수 정규분포를 여러개 곱한 형태가 된다.

$$
P(x_d \mid y = k) = \dfrac{1}{\sqrt{2\pi\sigma_{d,k}^2}} \exp \left(-\dfrac{(x_d-\mu_{d,k})^2}{2\sigma_{d,k}^2}\right)
$$

### 베르누이분포 가능도 모형

각각의 $$x = (x_1, \dots, x_D)$$ 의 각 원소가 0 또는 1이라는 값만을 가질 수 있다. 즉 독립변수는 D개의 독립적인 베르누이 확률변수이다. 모수 $$\mu_d$$ 는 동전 d 마다 다르다. 클래스 $$y=k(k=1, \dots, K)$$ 마다 $$x_d$$ 가 1이 될 확률이 다르다. 즉 동전의 모수 $$\mu_{d,k}$$ 는 동전 d마다 다르고 클래스 k마다도 다르다. 즉 전체 $$D$$ x $$K$$ 의 동전이 존재하며 같은 클래스에 속하는 D개의 동전이 하나의 동전 세트를 구성하고 이러한 동전 세트가 K개 있다고 생각할 수 있다. 

- 가능도 함수 

$$
P(x_d \mid y = k) = \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}
$$

- 모형 수식

$$
P(x_1, \ldots, x_D \mid y = k) 
= \prod_{d=1}^D \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}
$$

동전 세트마다 확률 특성이 다르므로 베르누이분포 가능도 모형을 기반으로 하는 나이브베이즈 모형은 동전세트를 N번 던진 결과로부터 1, $$\dots$$ , K 중 어느 동전 세트를 던졌는지를 찾아내는 모형이라고 할 수 있다. 

### 다항분포 가능도 모형

$$x$$ 벡터가 다항분포의 표본이라고 가정한다. (D개의 면을 가지는 주사위를 $$\sum_{d=1}^D x_d$$ 번 던져서 나온 결과로 본다) 예를 들어 $$x = (1, 4, 0, 5)$$ 같다면, 4면체 주사위를 10번 던져서 1인 면이 1번, 2인 면이 4번, 4인 면이 5번 나온 결과로 해석한다. 각 클래스마다 주사위가 다르다고 가정하므로 K개의 클래스를 구분하는 문제에서는 D개의 면을 가진 주사위가 K개 있다고 본다.  (독립은 아님! 원래 곱으로 표현되어 있음)

$$
P(x_1, \ldots, x_D \mid y = k) 
\;\; \propto \;\; \prod_{d=1}^D \mu_{d,k}^{x_{d,k}} \\
\sum_{d=1}^{D} \mu_{d,k} = 1
$$

주사위를 던진 결과로부터 $$1, \dots, K $$ 중 어느 주사위를 던졌는지를 찾아내는 모형이라고 할 수 있다. 

### 베르누이분포 나이브베이즈 모형

독립변수가 0또는 1의 값을 가지면 베르누이 나이브베이즈 모형을 사용한다.

$$
\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)
$$

$$N_k$$ 은 클래스 $$k$$ 에 대해 동전을 던진 총 횟수이다. 표본 데이터의 수가 적은 경우에는 모수에 대해 스무딩(smoothing) 을 할 수 있다.

### 스무딩 smoothing

표본 데이터의 수가 적은 경우에는 극단적인 모수 추정값이 나올 수 있다.ㅌ 하지만 현실적으로는 실제 모수값이 극단적인 값이 나올 가능성이 적다.

따라서 베르누이 모수경우 0.5인 가장 일반적인 경우를 가정하여 0이 나오는 경우와 1이 나오는 경우 두개의 가상 표본 데이터를 추가한다. 그러면 0과 1 같은 극단적인 추정값이 0.5 가까운 값으로 변한다. 이를 라플라스 스무딩(Laplace smoothing) 또는 애드원(Add-One) 스무딩이라고 한다.

$$
\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + 2 \alpha}
$$



